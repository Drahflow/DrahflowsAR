package name.drahflow.ar;

import android.app.Activity;
import android.app.ActivityManager;
import android.content.Context;
import android.content.pm.ConfigurationInfo;
import android.os.Bundle;
import android.os.Looper;
import android.os.Handler;
import android.os.HandlerThread;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.FloatBuffer;
import java.util.Arrays;
import javax.microedition.khronos.egl.EGLConfig;
import javax.microedition.khronos.opengles.GL10;
import android.app.Activity;
import android.hardware.Sensor;
import android.hardware.SensorEvent;
import android.hardware.SensorEventListener;
import android.hardware.SensorManager;
import android.opengl.GLSurfaceView;
import android.opengl.GLES20;
import android.opengl.GLES11Ext;
import android.opengl.Matrix;
import android.opengl.GLUtils;
import android.os.Bundle;
import android.os.SystemClock;
import android.util.Log;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.SurfaceTexture;
import android.view.View;
import android.view.Surface;
import android.view.Window;
import android.hardware.camera2.CameraManager;
import android.hardware.camera2.CameraAccessException;
import android.hardware.camera2.CameraDevice;
import android.hardware.camera2.CameraCaptureSession;
import android.hardware.camera2.CaptureRequest;
import android.hardware.camera2.TotalCaptureResult;
import android.hardware.camera2.CaptureFailure;
import android.hardware.camera2.CaptureResult;
import android.hardware.camera2.params.RggbChannelVector;
import com.epson.moverio.btcontrol.DisplayControl;

/**
 * Wrapper activity demonstrating the use of the new
 * {@link SensorEvent#values rotation vector sensor}
 * ({@link Sensor#TYPE_ROTATION_VECTOR TYPE_ROTATION_VECTOR}).
 * 
 * @see Sensor
 * @see SensorEvent
 * @see SensorManager
 * 
 */
public class DrahflowsAR extends Activity {
    private GLSurfaceView mGLSurfaceView;
    private SensorManager mSensorManager;
    private LessonThreeRenderer mRenderer;
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

				// disable non-3D elements
				requestWindowFeature(Window.FEATURE_NO_TITLE);
				View view = getWindow().getDecorView();
				view.setSystemUiVisibility(View.SYSTEM_UI_FLAG_HIDE_NAVIGATION | View.SYSTEM_UI_FLAG_FULLSCREEN | View.SYSTEM_UI_FLAG_IMMERSIVE_STICKY);

        // Get an instance of the SensorManager
        mSensorManager = (SensorManager)getSystemService(SENSOR_SERVICE);
        // Create our Preview view and set it as the content of our
        // Activity
				final ActivityManager activityManager = (ActivityManager) getSystemService(Context.ACTIVITY_SERVICE);
				final ConfigurationInfo configurationInfo = activityManager.getDeviceConfigurationInfo();
				final boolean supportsEs2 = configurationInfo.reqGlEsVersion >= 0x20000;
				if(!supportsEs2) {
					throw new RuntimeException("ES2 not supported, this is hopeless");
				}

        mRenderer = new LessonThreeRenderer();
        mGLSurfaceView = new GLSurfaceView(this);
				mGLSurfaceView.setEGLContextClientVersion(2);
        mGLSurfaceView.setRenderer(mRenderer);
        setContentView(mGLSurfaceView);

        int[] someTexs = new int[1];
        GLES20.glGenTextures ( 1, someTexs, 0 );
				mCameraTextureID = someTexs[0];

        GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, mCameraTextureID);
        GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, GLES20.GL_TEXTURE_WRAP_S, GLES20.GL_CLAMP_TO_EDGE);
        GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, GLES20.GL_TEXTURE_WRAP_T, GLES20.GL_CLAMP_TO_EDGE);
        GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_LINEAR);
        GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_LINEAR);
				mCameraTexture = new SurfaceTexture(mCameraTextureID);
    }

		private CameraDevice mCamera;
		private HandlerThread mBackgroundThread;
		private Handler mBackgroundHandler;
		private SurfaceTexture mCameraTexture;
		private int mCameraTextureID;

    @Override
    protected void onResume() {
        // Ideally a game should implement onResume() and onPause()
        // to take appropriate action when the activity looses focus
        super.onResume();
        mRenderer.start();
        mGLSurfaceView.onResume();
				new DisplayControl(this).setMode(DisplayControl. DISPLAY_MODE_3D, false);

				// mBackgroundThread = new HandlerThread("Camera Processing");
				// mBackgroundThread.start();
				// mBackgroundHandler = new Handler(mBackgroundThread.getLooper());
				mBackgroundHandler = new Handler(); // FIXME

				CameraManager cameraManager = (CameraManager)getSystemService(Context.CAMERA_SERVICE);
				try {
					String[] cameras = cameraManager.getCameraIdList();

					for(int i = 0; i < cameras.length; ++i) {
						Log.e("Holo", "Camera: " + cameras[i]);
					}

					cameraManager.openCamera(cameras[0], new CameraDevice.StateCallback() {
						public void onOpened(CameraDevice c) {
							mCamera = c;

							mCameraTexture.setDefaultBufferSize(1920, 1080);
							final Surface surface = new Surface(mCameraTexture);

							try {
								c.createCaptureSession(Arrays.asList(surface),
										new CameraCaptureSession.StateCallback() {
											@Override
											public void onConfigured(CameraCaptureSession cameraCaptureSession) {
												// The camera is already closed
												if (mCamera == null) {
													return;
												}

												try {
													// Finally, we start displaying the camera preview.
													CaptureRequest.Builder captureRequestBuilder = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);
													captureRequestBuilder.addTarget(surface);
													captureRequestBuilder.set(CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_AUTO);
													captureRequestBuilder.set(CaptureRequest.COLOR_CORRECTION_MODE, CaptureRequest.COLOR_CORRECTION_MODE_TRANSFORM_MATRIX);
													captureRequestBuilder.set(CaptureRequest.COLOR_CORRECTION_GAINS, new RggbChannelVector(20.0f, 5.0f, 5.0f, 5.0f));
													captureRequestBuilder.set(CaptureRequest.SENSOR_EXPOSURE_TIME, 20000000l);  // in usecs
													captureRequestBuilder.set(CaptureRequest.SENSOR_SENSITIVITY, 50000);
													CaptureRequest captureRequest = captureRequestBuilder.build();

													cameraCaptureSession.setRepeatingRequest(captureRequest, new CameraCaptureSession.CaptureCallback() {
														public void onCaptureBufferLost(CameraCaptureSession session, CaptureRequest request, Surface target, long frameNumber) { }
														public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request, TotalCaptureResult result) { }
														public void onCaptureFailed(CameraCaptureSession session, CaptureRequest request, CaptureFailure failure) { }
														public void onCaptureProgressed(CameraCaptureSession session, CaptureRequest request, CaptureResult partialResult) { }
														public void onCaptureSequenceAborted(CameraCaptureSession session, int sequenceId) { }
														public void onCaptureSequenceCompleted(CameraCaptureSession session, int sequenceId, long frameNumber) { }
														public void onCaptureStarted(CameraCaptureSession session, CaptureRequest request, long timestamp, long frameNumber) { }
													}, mBackgroundHandler);
												} catch (CameraAccessException e) {
													e.printStackTrace();
												}
											}

											@Override
											public void onConfigureFailed(CameraCaptureSession cameraCaptureSession) {
												Log.e("Holo", "Capture Config failed");
											}
										},
										null);
							} catch (CameraAccessException e) {
								e.printStackTrace();
							}
						}

						public void onError(CameraDevice c, int error) {
							c.close();
							mCamera = null;
						}
						public void onDisconnected(CameraDevice c) {
							c.close();
							mCamera = null;
						}
						public void onClosed(CameraDevice c) {
							mCamera = null;
						}
					}, mBackgroundHandler);
				} catch(CameraAccessException cae) {
					throw new Error("May not open camera", cae);
				}
    }
    @Override
    protected void onPause() {
        // Ideally a game should implement onResume() and onPause()
        // to take appropriate action when the activity looses focus
        super.onPause();
        mRenderer.stop();
        mGLSurfaceView.onPause();
				new DisplayControl(this).setMode(DisplayControl. DISPLAY_MODE_2D, false);

				if(mCamera != null) {
					mCamera.close();
				}

				mBackgroundThread.getLooper().quitSafely();
				mBackgroundHandler = null;
    }

		/**
		 * This class implements our custom renderer. Note that the GL10 parameter passed in is unused for OpenGL ES 2.0
		 * renderers -- the static class GLES20 is used instead.
		 */
		public class LessonTwoRenderer implements GLSurfaceView.Renderer {
			/** Used for debug logs. */
			private static final String TAG = "LessonTwoRenderer";
			
			/**
			 * Store the model matrix. This matrix is used to move models from object space (where each model can be thought
			 * of being located at the center of the universe) to world space.
			 */
			private float[] mModelMatrix = new float[16];

			/**
			 * Store the view matrix. This can be thought of as our camera. This matrix transforms world space to eye space;
			 * it positions things relative to our eye.
			 */
			private float[] mLookAtMatrix = new float[16];
			private float[] mViewMatrix = new float[16];

			/** Store the projection matrix. This is used to project the scene onto a 2D viewport. */
			private float[] mProjectionMatrix = new float[16];
			
			/** Allocate storage for the final combined matrix. This will be passed into the shader program. */
			private float[] mMVPMatrix = new float[16];
			
			/** 
			 * Stores a copy of the model matrix specifically for the light position.
			 */
			private float[] mLightModelMatrix = new float[16];	
			
			/** Store our model data in a float buffer. */
			private final FloatBuffer mCubePositions;
			private final FloatBuffer mCubeColors;
			private final FloatBuffer mCubeNormals;
			
			/** This will be used to pass in the transformation matrix. */
			private int mMVPMatrixHandle;
			
			/** This will be used to pass in the modelview matrix. */
			private int mMVMatrixHandle;
			
			/** This will be used to pass in the light position. */
			private int mLightPosHandle;
			
			/** This will be used to pass in model position information. */
			private int mPositionHandle;
			
			/** This will be used to pass in model color information. */
			private int mColorHandle;
			
			/** This will be used to pass in model normal information. */
			private int mNormalHandle;

			/** This will be used to pass in camera texture. */
			private int mCameraDataHandle;

			/** How many bytes per float. */
			private final int mBytesPerFloat = 4;	
			
			/** Size of the position data in elements. */
			private final int mPositionDataSize = 3;	
			
			/** Size of the color data in elements. */
			private final int mColorDataSize = 4;	
			
			/** Size of the normal data in elements. */
			private final int mNormalDataSize = 3;

			/** Store our model data in a float buffer. */
			private final FloatBuffer mCubeTextureCoordinates;
			 
			/** This will be used to pass in the texture. */
			private int mTextureUniformHandle;
			 
			/** This will be used to pass in model texture coordinate information. */
			private int mTextureCoordinateHandle;
			 
			/** Size of the texture coordinate data in elements. */
			private final int mTextureCoordinateDataSize = 2;
			 
			/** This is a handle to our texture data. */
			// private int mTextureDataHandle;
			
			/** Used to hold a light centered on the origin in model space. We need a 4th coordinate so we can get translations to work when
			 *  we multiply this by our transformation matrices. */
			private final float[] mLightPosInModelSpace = new float[] {0.0f, 0.0f, 0.0f, 1.0f};
			
			/** Used to hold the current position of the light in world space (after transformation via model matrix). */
			private final float[] mLightPosInWorldSpace = new float[4];
			
			/** Used to hold the transformed position of the light in eye space (after transformation via modelview matrix) */
			private final float[] mLightPosInEyeSpace = new float[4];
			
			/** This is a handle to our per-vertex cube shading program. */
			private int mPerVertexProgramHandle;
				
			/** This is a handle to our light point program. */
			private int mPointProgramHandle;	
								
      private Sensor mRotationVectorSensor;
      private final float[] mRotationMatrix = new float[16];
			private final float[] mPosition = new float[]{0f, 0f, 0f, 0f};

			/**
			 * Initialize the model data.
			 */
			public LessonTwoRenderer()
			{	
				// Define points for a cube.		
				
				// X, Y, Z
				final float[] cubePositionData =
				{
						// In OpenGL counter-clockwise winding is default. This means that when we look at a triangle, 
						// if the points are counter-clockwise we are looking at the "front". If not we are looking at
						// the back. OpenGL has an optimization where all back-facing triangles are culled, since they
						// usually represent the backside of an object and aren't visible anyways.
						
						// Front face
						-1.0f, 1.0f, 1.0f,				
						-1.0f, -1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 
						-1.0f, -1.0f, 1.0f, 				
						1.0f, -1.0f, 1.0f,
						1.0f, 1.0f, 1.0f,
						
						// Right face
						1.0f, 1.0f, 1.0f,				
						1.0f, -1.0f, 1.0f,
						1.0f, 1.0f, -1.0f,
						1.0f, -1.0f, 1.0f,				
						1.0f, -1.0f, -1.0f,
						1.0f, 1.0f, -1.0f,
						
						// Back face
						1.0f, 1.0f, -1.0f,				
						1.0f, -1.0f, -1.0f,
						-1.0f, 1.0f, -1.0f,
						1.0f, -1.0f, -1.0f,				
						-1.0f, -1.0f, -1.0f,
						-1.0f, 1.0f, -1.0f,
						
						// Left face
						-1.0f, 1.0f, -1.0f,				
						-1.0f, -1.0f, -1.0f,
						-1.0f, 1.0f, 1.0f, 
						-1.0f, -1.0f, -1.0f,				
						-1.0f, -1.0f, 1.0f, 
						-1.0f, 1.0f, 1.0f, 
						
						// Top face
						-1.0f, 1.0f, -1.0f,				
						-1.0f, 1.0f, 1.0f, 
						1.0f, 1.0f, -1.0f, 
						-1.0f, 1.0f, 1.0f, 				
						1.0f, 1.0f, 1.0f, 
						1.0f, 1.0f, -1.0f,
						
						// Bottom face
						1.0f, -1.0f, -1.0f,				
						1.0f, -1.0f, 1.0f, 
						-1.0f, -1.0f, -1.0f,
						1.0f, -1.0f, 1.0f, 				
						-1.0f, -1.0f, 1.0f,
						-1.0f, -1.0f, -1.0f,
				};	
				
				// R, G, B, A
				final float[] cubeColorData =
				{				
						// Front face (red)
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						
						// Right face (green)
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						
						// Back face (pink)
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						
						// Left face (yellow)
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						
						// Top face (cyan)
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						
						// Bottom face (magenta)
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f,				
						1.0f, 1.0f, 1.0f, 1.0f,
						1.0f, 1.0f, 1.0f, 1.0f
				};
				
				// X, Y, Z
				// The normal is used in light calculations and is a vector which points
				// orthogonal to the plane of the surface. For a cube model, the normals
				// should be orthogonal to the points of each face.
				final float[] cubeNormalData =
				{												
						// Front face
						0.0f, 0.0f, 1.0f,				
						0.0f, 0.0f, 1.0f,
						0.0f, 0.0f, 1.0f,
						0.0f, 0.0f, 1.0f,				
						0.0f, 0.0f, 1.0f,
						0.0f, 0.0f, 1.0f,
						
						// Right face 
						1.0f, 0.0f, 0.0f,				
						1.0f, 0.0f, 0.0f,
						1.0f, 0.0f, 0.0f,
						1.0f, 0.0f, 0.0f,				
						1.0f, 0.0f, 0.0f,
						1.0f, 0.0f, 0.0f,
						
						// Back face 
						0.0f, 0.0f, -1.0f,				
						0.0f, 0.0f, -1.0f,
						0.0f, 0.0f, -1.0f,
						0.0f, 0.0f, -1.0f,				
						0.0f, 0.0f, -1.0f,
						0.0f, 0.0f, -1.0f,
						
						// Left face 
						-1.0f, 0.0f, 0.0f,				
						-1.0f, 0.0f, 0.0f,
						-1.0f, 0.0f, 0.0f,
						-1.0f, 0.0f, 0.0f,				
						-1.0f, 0.0f, 0.0f,
						-1.0f, 0.0f, 0.0f,
						
						// Top face 
						0.0f, 1.0f, 0.0f,			
						0.0f, 1.0f, 0.0f,
						0.0f, 1.0f, 0.0f,
						0.0f, 1.0f, 0.0f,				
						0.0f, 1.0f, 0.0f,
						0.0f, 1.0f, 0.0f,
						
						// Bottom face 
						0.0f, -1.0f, 0.0f,			
						0.0f, -1.0f, 0.0f,
						0.0f, -1.0f, 0.0f,
						0.0f, -1.0f, 0.0f,				
						0.0f, -1.0f, 0.0f,
						0.0f, -1.0f, 0.0f
				};

        // S, T (or X, Y)
		    // Texture coordinate data.
		    // Because images have a Y axis pointing downward (values increase as you move down the image) while
		    // OpenGL has a Y axis pointing upward, we adjust for that here by flipping the Y axis.
		    // What's more is that the texture coordinates are the same for every face.
		    final float[] cubeTextureCoordinateData =
		    {												
		    		// Front face
		    		0.0f, 0.0f, 				
		    		0.0f, 1.0f,
		    		1.0f, 0.0f,
		    		0.0f, 1.0f,
		    		1.0f, 1.0f,
		    		1.0f, 0.0f,				
		    		
		    		// Right face 
		    		0.0f, 0.0f, 				
		    		0.0f, 1.0f,
		    		1.0f, 0.0f,
		    		0.0f, 1.0f,
		    		1.0f, 1.0f,
		    		1.0f, 0.0f,	
		    		
		    		// Back face 
		    		0.0f, 0.0f, 				
		    		0.0f, 1.0f,
		    		1.0f, 0.0f,
		    		0.0f, 1.0f,
		    		1.0f, 1.0f,
		    		1.0f, 0.0f,	
		    		
		    		// Left face 
		    		0.0f, 0.0f, 				
		    		0.0f, 1.0f,
		    		1.0f, 0.0f,
		    		0.0f, 1.0f,
		    		1.0f, 1.0f,
		    		1.0f, 0.0f,	
		    		
		    		// Top face 
		    		0.0f, 0.0f, 				
		    		0.0f, 1.0f,
		    		1.0f, 0.0f,
		    		0.0f, 1.0f,
		    		1.0f, 1.0f,
		    		1.0f, 0.0f,	
		    		
		    		// Bottom face 
		    		0.0f, 0.0f, 				
		    		0.0f, 1.0f,
		    		1.0f, 0.0f,
		    		0.0f, 1.0f,
		    		1.0f, 1.0f,
		    		1.0f, 0.0f
		    };

				// Initialize the buffers.
				mCubePositions = ByteBuffer.allocateDirect(cubePositionData.length * mBytesPerFloat)
						.order(ByteOrder.nativeOrder()).asFloatBuffer();							
				mCubePositions.put(cubePositionData).position(0);		
				
				mCubeColors = ByteBuffer.allocateDirect(cubeColorData.length * mBytesPerFloat)
						.order(ByteOrder.nativeOrder()).asFloatBuffer();							
				mCubeColors.put(cubeColorData).position(0);
				
				mCubeNormals = ByteBuffer.allocateDirect(cubeNormalData.length * mBytesPerFloat)
						.order(ByteOrder.nativeOrder()).asFloatBuffer();							
				mCubeNormals.put(cubeNormalData).position(0);

				mCubeTextureCoordinates = ByteBuffer.allocateDirect(cubeTextureCoordinateData.length * mBytesPerFloat)
						.order(ByteOrder.nativeOrder()).asFloatBuffer();
				mCubeTextureCoordinates.put(cubeTextureCoordinateData).position(0);

        mRotationVectorSensor = mSensorManager.getDefaultSensor(
                Sensor.TYPE_ROTATION_VECTOR);
        mRotationMatrix[ 0] = 1;
        mRotationMatrix[ 4] = 1;
        mRotationMatrix[ 8] = 1;
        mRotationMatrix[12] = 1;
			}
			
			protected String getVertexShader()
			{
				throw new RuntimeException("Not used");
			}
			
			protected String getFragmentShader()
			{
				throw new RuntimeException("Not used");
			}
			
			private SensorEventListener rotationListener = new SensorEventListener() {
				public void onSensorChanged(SensorEvent event) {
					System.arraycopy(sensorStateB, 0, sensorStateA, 0, 3);
					sensorStateATime = sensorStateBTime;
					System.arraycopy(event.values, 0, sensorStateB, 0, 3);
					sensorStateBTime = event.timestamp;
				}
				public void onAccuracyChanged(Sensor sensor, int accuracy) { }
			};

      public void start() {
          // enable our sensor when the activity is resumed, ask for
          // 10 ms updates.
          mSensorManager.registerListener(rotationListener, mRotationVectorSensor, 1000);
      }

      public void stop() {
          // make sure to turn our sensor off when the activity is paused
          mSensorManager.unregisterListener(rotationListener);
      }

			private float[] sensorStateA = new float[3];
			private long sensorStateATime = 0;
			private float[] sensorStateB = new float[3];
			private long sensorStateBTime = 1;
			private float[] sensorState = new float[3];

			@Override
			public void onSurfaceCreated(GL10 glUnused, EGLConfig config) 
			{
				// Set the background clear color to black.
				GLES20.glClearColor(0.0f, 0.0f, 0.0f, 0.0f);
				
				// Use culling to remove back faces.
				GLES20.glEnable(GLES20.GL_CULL_FACE);
				
				// Enable depth testing
				GLES20.glEnable(GLES20.GL_DEPTH_TEST);
					
				final String vertexShader = getVertexShader();   		
				final String fragmentShader = getFragmentShader();			
				
				final int vertexShaderHandle = compileShader(GLES20.GL_VERTEX_SHADER, vertexShader);		
				final int fragmentShaderHandle = compileShader(GLES20.GL_FRAGMENT_SHADER, fragmentShader);		
				
				mPerVertexProgramHandle = createAndLinkProgram(vertexShaderHandle, fragmentShaderHandle, 
						new String[] {"a_Position",  "a_Color", "a_Normal", "a_TexCoordinate"});
						
						// Define a simple shader program for our point.
						final String pointVertexShader =
							"uniform mat4 u_MVPMatrix;      \n"		
							+	"attribute vec4 a_Position;     \n"		
							+ "void main()                    \n"
							+ "{                              \n"
							+ "   gl_Position = u_MVPMatrix   \n"
							+ "               * a_Position;   \n"
							+ "   gl_PointSize = 5.0;         \n"
							+ "}                              \n";
						
						final String pointFragmentShader = 
							"precision mediump float;       \n"					          
							+ "void main()                    \n"
							+ "{                              \n"
							+ "   gl_FragColor = vec4(1.0,    \n" 
							+ "   1.0, 1.0, 1.0);             \n"
							+ "}                              \n";
						
						final int pointVertexShaderHandle = compileShader(GLES20.GL_VERTEX_SHADER, pointVertexShader);
						final int pointFragmentShaderHandle = compileShader(GLES20.GL_FRAGMENT_SHADER, pointFragmentShader);
						mPointProgramHandle = createAndLinkProgram(pointVertexShaderHandle, pointFragmentShaderHandle, 
								new String[] {"a_Position"});                 

						// Load the texture
						// mTextureDataHandle = loadTexture(DrahflowsAR.this, R.drawable.bumpy_bricks_public_domain);
			}	
				
			private int width;
			private int height;

			@Override
			public void onSurfaceChanged(GL10 glUnused, int width, int height) 
			{
				// Set the OpenGL viewport to the same size as the surface.
				GLES20.glViewport(0, 0, width / 2, height);
				this.width = width;
				this.height = height;

				// Create a new perspective projection matrix. The height will stay the same
				// while the width will vary as per aspect ratio.
				final float ratio = (float) width / height;
				final float zoom = 110f;
				final float left = -ratio / zoom;
				final float right = ratio / zoom;
				final float bottom = -1.0f / zoom;
				final float top = 1.0f / zoom;
				final float near = 0.1f;
				final float far = 10.0f;
				
				Matrix.frustumM(mProjectionMatrix, 0, left, right, bottom, top, near, far);
			}	

			@Override
			public void onDrawFrame(GL10 glUnused) 
			{
						// Position the eye in front of the origin.
						final float eyeX = 0.0f;
						final float eyeY = 0.0f;
						final float eyeZ = 0.0f;

						// We are looking toward the distance
						final float lookX = 0.0f;
						final float lookY = 0.0f;
						final float lookZ = -5.0f;

						// Set our up vector. This is where our head would be pointing were we holding the camera.
						final float upX = 0.0f;
						final float upY = 1.0f;
						final float upZ = 0.0f;

						GLES20.glViewport(0, 0, width, height);
						GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT | GLES20.GL_DEPTH_BUFFER_BIT);			        

						mCameraTexture.updateTexImage();
										
						// Do a complete rotation every 10 seconds.
						long time = SystemClock.uptimeMillis() % 20000L;        
						float angleInDegrees = (360.0f / 10000.0f) * ((int) time);                
						
						// Set program handles for cube drawing.
						mMVPMatrixHandle = GLES20.glGetUniformLocation(mPerVertexProgramHandle, "u_MVPMatrix");
						mMVMatrixHandle = GLES20.glGetUniformLocation(mPerVertexProgramHandle, "u_MVMatrix"); 
						mLightPosHandle = GLES20.glGetUniformLocation(mPerVertexProgramHandle, "u_LightPos");
						mPositionHandle = GLES20.glGetAttribLocation(mPerVertexProgramHandle, "a_Position");
						mColorHandle = GLES20.glGetAttribLocation(mPerVertexProgramHandle, "a_Color");
						mNormalHandle = GLES20.glGetAttribLocation(mPerVertexProgramHandle, "a_Normal"); 
						mTextureCoordinateHandle = GLES20.glGetAttribLocation(mPerVertexProgramHandle, "a_TexCoordinate");
						mCameraDataHandle = GLES20.glGetAttribLocation(mPerVertexProgramHandle, "u_CameraData");
						
						estimateRotationAt(60000000 + SystemClock.elapsedRealtimeNanos());
						SensorManager.getRotationMatrixFromVector(mRotationMatrix, sensorState);

						// Set the view matrix. This matrix can be said to represent the camera position.
						// NOTE: In OpenGL 1, a ModelView matrix is used, which is a combination of a model and
						// view matrix. In OpenGL 2, we can keep track of these matrices separately if we choose.
						GLES20.glViewport(0, 0, width / 2, height);
						Matrix.setLookAtM(mLookAtMatrix, 0, eyeX - 0.03f, eyeY, eyeZ, lookX, lookY, lookZ, upX, upY, upZ);
						drawEyeView(angleInDegrees);

						GLES20.glViewport(width / 2, 0, width / 2, height);
						Matrix.setLookAtM(mLookAtMatrix, 0, eyeX + 0.03f, eyeY, eyeZ, lookX, lookY, lookZ, upX, upY, upZ);
						drawEyeView(angleInDegrees);
			}

			/**
			 * Estimates rotation at given time.
			 *
			 * Output in sensorState[]
			 */
			private void estimateRotationAt(long timestamp) {
						final float dx = sensorStateB[0] - sensorStateA[0];
						final float dy = sensorStateB[1] - sensorStateA[1];
						final float dz = sensorStateB[2] - sensorStateA[2];

						if(Math.sqrt(dx * dx + dy * dy + dz * dz) > 0.001) {
							float factor = (float)(timestamp - sensorStateBTime) /
								(sensorStateBTime - sensorStateATime);

							sensorState[0] = sensorStateB[0] + factor * (sensorStateB[0] - sensorStateA[0]);
							sensorState[1] = sensorStateB[1] + factor * (sensorStateB[1] - sensorStateA[1]);
							sensorState[2] = sensorStateB[2] + factor * (sensorStateB[2] - sensorStateA[2]);
						} else {
							sensorState[0] = sensorStateB[0];
							sensorState[1] = sensorStateB[1];
							sensorState[2] = sensorStateB[2];
						}
			}

			private void drawEyeView(float angleInDegrees) {
						// convert the rotation-vector to a 4x4 matrix. the matrix
						// is interpreted by Open GL as the inverse of the
						// rotation-vector, which is what we want.

						// don't handle AR movement, TODO: remove
						Matrix.setIdentityM(mRotationMatrix, 0);
						Matrix.multiplyMM(mViewMatrix, 0, mLookAtMatrix, 0, mRotationMatrix, 0);
						Matrix.translateM(mViewMatrix, 0, mPosition[0], mPosition[1], mPosition[2]);

						// handle movement in AR, TODO: reenable
						// Matrix.multiplyMM(mViewMatrix, 0, mLookAtMatrix, 0, mRotationMatrix, 0);
						// Matrix.translateM(mViewMatrix, 0, mPosition[0], mPosition[1], mPosition[2]);
						// Matrix.rotateM(mViewMatrix, 0, 90, 0.0f, 1.0f, 0.0f);

						// Calculate position of the light. Rotate and then push into the distance.
						Matrix.setIdentityM(mLightModelMatrix, 0);
						Matrix.translateM(mLightModelMatrix, 0, -40.0f, 0.0f, 0.0f);

						Matrix.multiplyMV(mLightPosInWorldSpace, 0, mLightModelMatrix, 0, mLightPosInModelSpace, 0);
						Matrix.multiplyMV(mLightPosInEyeSpace, 0, mViewMatrix, 0, mLightPosInWorldSpace, 0);                        
						
						// Set our per-vertex lighting program.
						GLES20.glUseProgram(mPerVertexProgramHandle);
						
						// Draw some cubes.        
						Matrix.setIdentityM(mModelMatrix, 0);
						Matrix.translateM(mModelMatrix, 0, 0.0f, 0.0f, -0.5f);
						Matrix.rotateM(mModelMatrix, 0, 90, 0.0f, 1.0f, 0.0f);
						Matrix.scaleM(mModelMatrix, 0, 0.03f, 0.05f, 0.05f);
						drawCube();

						// Draw a point to indicate the light.
						GLES20.glUseProgram(mPointProgramHandle);        
						drawLight();
			}				
			
			/**
			 * Draws a cube.
			 */			
			private void drawCube()
			{		
				// Pass in the position information
				mCubePositions.position(0);		
						GLES20.glVertexAttribPointer(mPositionHandle, mPositionDataSize, GLES20.GL_FLOAT, false,
								0, mCubePositions);        
										
						GLES20.glEnableVertexAttribArray(mPositionHandle);        
						
						// Pass in the color information
						mCubeColors.position(0);
						GLES20.glVertexAttribPointer(mColorHandle, mColorDataSize, GLES20.GL_FLOAT, false,
								0, mCubeColors);        
						
						GLES20.glEnableVertexAttribArray(mColorHandle);
						
						// Pass in the normal information
						mCubeNormals.position(0);
						GLES20.glVertexAttribPointer(mNormalHandle, mNormalDataSize, GLES20.GL_FLOAT, false, 
								0, mCubeNormals);
						
						GLES20.glEnableVertexAttribArray(mNormalHandle);

						// Pass in the texture coordinate information
						mCubeTextureCoordinates.position(0);
						GLES20.glVertexAttribPointer(mTextureCoordinateHandle, mTextureCoordinateDataSize, GLES20.GL_FLOAT, false, 
								0, mCubeTextureCoordinates);
						
						GLES20.glEnableVertexAttribArray(mTextureCoordinateHandle);

						GLES20.glActiveTexture(GLES20.GL_TEXTURE0);
						GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, mCameraTextureID);
						GLES20.glUniform1i(mCameraDataHandle, 0);

						// This multiplies the view matrix by the model matrix, and stores the result in the MVP matrix
						// (which currently contains model * view).
						Matrix.multiplyMM(mMVPMatrix, 0, mViewMatrix, 0, mModelMatrix, 0);   
						
						// Pass in the modelview matrix.
						GLES20.glUniformMatrix4fv(mMVMatrixHandle, 1, false, mMVPMatrix, 0);                
						
						// This multiplies the modelview matrix by the projection matrix, and stores the result in the MVP matrix
						// (which now contains model * view * projection).
						Matrix.multiplyMM(mMVPMatrix, 0, mProjectionMatrix, 0, mMVPMatrix, 0);

						// Pass in the combined matrix.
						GLES20.glUniformMatrix4fv(mMVPMatrixHandle, 1, false, mMVPMatrix, 0);
						
						// Pass in the light position in eye space.        
						GLES20.glUniform3f(mLightPosHandle, mLightPosInEyeSpace[0], mLightPosInEyeSpace[1], mLightPosInEyeSpace[2]);
						
						// Draw the cube.
						GLES20.glDrawArrays(GLES20.GL_TRIANGLES, 0, 36);                               
			}	
			
			/**
			 * Draws a point representing the position of the light.
			 */
			private void drawLight()
			{
				final int pointMVPMatrixHandle = GLES20.glGetUniformLocation(mPointProgramHandle, "u_MVPMatrix");
						final int pointPositionHandle = GLES20.glGetAttribLocation(mPointProgramHandle, "a_Position");
						
				// Pass in the position.
				GLES20.glVertexAttrib3f(pointPositionHandle, mLightPosInModelSpace[0], mLightPosInModelSpace[1], mLightPosInModelSpace[2]);

				// Since we are not using a buffer object, disable vertex arrays for this attribute.
						GLES20.glDisableVertexAttribArray(pointPositionHandle);  
				
				// Pass in the transformation matrix.
				Matrix.multiplyMM(mMVPMatrix, 0, mViewMatrix, 0, mLightModelMatrix, 0);
				Matrix.multiplyMM(mMVPMatrix, 0, mProjectionMatrix, 0, mMVPMatrix, 0);
				GLES20.glUniformMatrix4fv(pointMVPMatrixHandle, 1, false, mMVPMatrix, 0);
				
				// Draw the point.
				GLES20.glDrawArrays(GLES20.GL_POINTS, 0, 1);
			}
			
			/** 
			 * Helper function to compile a shader.
			 * 
			 * @param shaderType The shader type.
			 * @param shaderSource The shader source code.
			 * @return An OpenGL handle to the shader.
			 */
			private int compileShader(final int shaderType, final String shaderSource) 
			{
				int shaderHandle = GLES20.glCreateShader(shaderType);

				if (shaderHandle != 0) 
				{
					// Pass in the shader source.
					GLES20.glShaderSource(shaderHandle, shaderSource);

					// Compile the shader.
					GLES20.glCompileShader(shaderHandle);

					// Get the compilation status.
					final int[] compileStatus = new int[1];
					GLES20.glGetShaderiv(shaderHandle, GLES20.GL_COMPILE_STATUS, compileStatus, 0);

					// If the compilation failed, delete the shader.
					if (compileStatus[0] == 0) 
					{
						Log.e(TAG, "Error compiling shader: " + GLES20.glGetShaderInfoLog(shaderHandle));
						GLES20.glDeleteShader(shaderHandle);
						shaderHandle = 0;
					}
				}

				if (shaderHandle == 0)
				{			
					throw new RuntimeException("Error creating shader.");
				}
				
				return shaderHandle;
			}	
			
			/**
			 * Helper function to compile and link a program.
			 * 
			 * @param vertexShaderHandle An OpenGL handle to an already-compiled vertex shader.
			 * @param fragmentShaderHandle An OpenGL handle to an already-compiled fragment shader.
			 * @param attributes Attributes that need to be bound to the program.
			 * @return An OpenGL handle to the program.
			 */
			private int createAndLinkProgram(final int vertexShaderHandle, final int fragmentShaderHandle, final String[] attributes) 
			{
				int programHandle = GLES20.glCreateProgram();
				
				if (programHandle != 0) 
				{
					// Bind the vertex shader to the program.
					GLES20.glAttachShader(programHandle, vertexShaderHandle);			

					// Bind the fragment shader to the program.
					GLES20.glAttachShader(programHandle, fragmentShaderHandle);
					
					// Bind attributes
					if (attributes != null)
					{
						final int size = attributes.length;
						for (int i = 0; i < size; i++)
						{
							GLES20.glBindAttribLocation(programHandle, i, attributes[i]);
						}						
					}
					
					// Link the two shaders together into a program.
					GLES20.glLinkProgram(programHandle);

					// Get the link status.
					final int[] linkStatus = new int[1];
					GLES20.glGetProgramiv(programHandle, GLES20.GL_LINK_STATUS, linkStatus, 0);

					// If the link failed, delete the program.
					if (linkStatus[0] == 0) 
					{				
						Log.e(TAG, "Error compiling program: " + GLES20.glGetProgramInfoLog(programHandle));
						GLES20.glDeleteProgram(programHandle);
						programHandle = 0;
					}
				}
				
				if (programHandle == 0)
				{
					throw new RuntimeException("Error creating program.");
				}
				
				return programHandle;
			}
		}

		public class LessonThreeRenderer extends LessonTwoRenderer
		{
			protected String getVertexShader()
			{
				// Define our per-pixel lighting shader.
						final String perPixelVertexShader =
					"uniform mat4 u_MVPMatrix;      \n"		// A constant representing the combined model/view/projection matrix.
					+ "uniform mat4 u_MVMatrix;       \n"		// A constant representing the combined model/view matrix.
								
					+ "attribute vec4 a_Position;     \n"		// Per-vertex position information we will pass in.
					+ "attribute vec4 a_Color;        \n"		// Per-vertex color information we will pass in.
					+ "attribute vec3 a_Normal;       \n"		// Per-vertex normal information we will pass in.
					+ "attribute vec2 a_TexCoordinate; \n"		// Per-vertex normal information we will pass in.
					
					+ "varying vec3 v_Position;       \n"		// This will be passed into the fragment shader.
					+ "varying vec4 v_Color;          \n"		// This will be passed into the fragment shader.
					+ "varying vec3 v_Normal;         \n"		// This will be passed into the fragment shader.
					+ "varying vec2 v_TexCoordinate;  \n"		// This will be passed into the fragment shader.
					
				// The entry point for our vertex shader.  
					+ "void main()                                                \n" 	
					+ "{                                                          \n"
				// Transform the vertex into eye space.
					+ "   v_Position = vec3(u_MVMatrix * a_Position);             \n"
				// Pass through the color.
					+ "   v_Color = a_Color;                                      \n"
				// Pass through the texture.
					+ "   v_TexCoordinate = a_TexCoordinate;                      \n"
				// Transform the normal's orientation into eye space.
					+ "   v_Normal = vec3(u_MVMatrix * vec4(a_Normal, 0.0));      \n"
				// gl_Position is a special variable used to store the final position.
				// Multiply the vertex by the matrix to get the final point in normalized screen coordinates.
					+ "   gl_Position = u_MVPMatrix * a_Position;                 \n"      		  
					+ "}                                                          \n";      
				
				return perPixelVertexShader;
			}
			
			protected String getFragmentShader()
			{
				final String perPixelFragmentShader =
					"#extension GL_OES_EGL_image_external : require\n"  // Allow external (i.e. camera) textures
					+ "precision mediump float;       \n"		// Set the default precision to medium. We don't need as high of a 
															// precision in the fragment shader.
					+ "uniform vec3 u_LightPos;       \n"	    // The position of the light in eye space.
          + "uniform sampler2D u_Texture;   \n"     // The input texture.
          + "uniform samplerExternalOES u_CameraData;\n"
					+ "varying vec3 v_Position;		\n"		// Interpolated position for this fragment.
					+ "varying vec4 v_Color;          \n"		// This is the color from the vertex shader interpolated across the 
																// triangle per fragment.
					+ "varying vec2 v_TexCoordinate;  \n" // Interpolated texture coordinate per fragment.
					+ "varying vec3 v_Normal;         \n"		// Interpolated normal for this fragment.
					
				// The entry point for our fragment shader.
					+ "void main()                    \n"		
					+ "{                              \n"
					+ "   vec4 middle = texture2D(u_CameraData, v_TexCoordinate); \n"
					+ "   gl_FragColor = middle; \n"
					+ "}                                                                     \n";	
				
				return perPixelFragmentShader;
			}		
		}

		public static int loadTexture(final Context context, final int resourceId) {
				final int[] textureHandle = new int[1];
		 
				GLES20.glGenTextures(1, textureHandle, 0);
		 
				if (textureHandle[0] != 0)
				{
						final BitmapFactory.Options options = new BitmapFactory.Options();
						options.inScaled = false;   // No pre-scaling
		 
						// Read in the resource
						final Bitmap bitmap = BitmapFactory.decodeResource(context.getResources(), resourceId, options);
		 
						// Bind to the texture in OpenGL
						GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, textureHandle[0]);
		 
						// Set filtering
						GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_LINEAR);
						GLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_LINEAR);
		 
						// Load the bitmap into the bound texture.
						GLUtils.texImage2D(GLES20.GL_TEXTURE_2D, 0, bitmap, 0);
		 
						// Recycle the bitmap, since its data has been loaded into OpenGL.
						bitmap.recycle();
				}
		 
				if (textureHandle[0] == 0)
				{
						throw new RuntimeException("Error loading texture.");
				}
		 
				return textureHandle[0];
		}
}
